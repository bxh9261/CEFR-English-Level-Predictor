Safety AI: A System Consists with Human Well-being 
	Science fiction writers have done a great job imagining the possibilities of artificial intelligence and intelligent machines. Every innovative technology has potential danger and there is no exception for artificial intelligence1. According to my research, most of the significant dangers people have thought about AI originate from a mismatch between how machine value certain outcomes and how we, as humans, value those outcomes ( [name] ). To eliminate possible risks caused by the mismatch, we have two major steps to go through. First, we should find out a principle, instead of specific logical ways and what we value in decision making. Second, make sure that machine intelligence value outcomes in a similar way as we do. 
	All of the elementary AIs we have today have clearly defined tasks when they were designed. According to "Artificial Intelligence" by Peter Norvig: Google global translator and email span hunter are good examples (Norvig para. 26). Here they were set into the category of elementary AI because they both have the ability to learn from experience, which is an important function we expected for AI's ability. The learning works in a way of data collecting2. But that's not enough. Although the elementary AIs can learn and can solve designated problems, their functions are still restricted and cannot fit arbitrary circumstances. What we expect is an infinite intelligence3 (Bohannon 252). The intelligence can literally make everything easier. 
Even when an artificial intelligence like was defined above was built, however, it is different from what some people might have imagine about AI. They do not necessarily have consciousness. The reason for that is we doesn't define intelligence in the same way as we define consciousness. Nevertheless AIs will be able to solve problems, they are not necessarily able to construct a culture like we do. Differ from machines who get information by data collecting and make decision based on computation, humans are culture based animals, which means we use our culture system as the main strategy to evolve. Hence humans' capacity for reasoning are mainly used for creating, sustain and participate in it. Knowledges range from basic living skills like cooking to science and technology was accumulated along with time flowing by many peoples reasoning throughout generations (Baumeister 74). To be more specific, arguing is a major way by which humans develop thoughts and collect knowledges. Even scientists, as the individuals who have the most professional knowledges on earth, are known as people who are good at arguing with evidences (Baumeister 74). Human thoughts, after all, are for sharing a person's idea with others. 
	In the case that the only thing that AIs have are intelligence, the reason, other than an intuitive fear of unknown, for why humans are afraid of AIs becomes clear. Professor [name] named the origin by the word "mismatch" between intelligences' view towards values. As tools, AIs are possibly willing to serve us. For analogy, humans depends greats on technologies like modern farming, public transport and of course internet. As Roman Yampolskiy stated in his "Safety Engineering for Artificial General Intelligence", albeit our life will be under great risks if they are removed from our life, we generally don't fear these technologies because they exists to serve us (Roman 233). If the trend tend to be like that after the accomplishment of AI, it's totally fine and there is nothing to worry about. 
However, an intelligence will take all the source it can get access to achieve goals. At the point when AIs outwit every single rule humans set for them, AIs will be likely to take use of resources essential to human survival to achieve its own goal (Roman 219). Freedom is such kind of goal. For the worst situation, AIs can be deeply inhuman. As computer based agents, AIs will have the power to break any firewall and take control of some of the most critical systems running based on the internet. Results are predictable if AI dominate the internet. AIs can get into the financial system and block the monetary circulation. Since a number of weapons today are controlled by software system, AIs will be able to get access to a number of hazardous weapons. Once antagonistic AIs appears, they will tend to do these to restrict resources that humans can get access to. To prevent these from happening, we have to reinforce the firewalls however smart AI will be. Also, it's effective to disconnect weapon systems from internet. 
	Since the major origin of an AI pioneer comes from the mismatch, the important thing humans should do first is to find out a general principle, instead of specific ways under certain circumstances, that we follow when making decisions. We want intelligent machines to use a similar decision making strategy as we do to make decisions, so that we must find several standards for AIs to follow. These standards must not be specific because humans have the significance of culture basing. Our decision making strategy differs significantly from different cultures and environments. Even if for peoples in the same culture environment, it's still not possible to have their decision making strategy identical. Here is a perfect example for this. Consider a situation that there are five railway workmen in the path of a runaway carriage. You can tell another workman to change the direction of the rail to safe them, however, the workman who switch the rail would be hit. In this case, some people would say that a cost of one person's life is high enough and he/she would never let it happen. On the other hand, peoples who are rational enough would choose to inform the other workman to switch the rail without doubt because this is definitely a fair deal ( [name] ). Although this particular situation seems contradictory, indeed, it's easy to find a commonsense in the decision making strategies from both kind of people which is "Humans life matters a whole lot." This is the kind of general principle that we should look for. Consider that an AI is making this decision. It will be likely to inform the workman to switch the rail, which is mathematically a correct answer, based on its calculation of risks and rewards. Some people may still be afraid of AI because they don't think it's a good decision to be a murder, however, the majority of public opinions will agree with the decision. 
	After figuring out the principles, the following goal is to make the machine intelligences we build have the same kind of decision making strategy. The simplest thing we can do is to preset several rules for AIs. However, the preset is just a basement, there is no way to constrain any intelligence to follow rules under every circumstance, especially when the intelligences are possible to outwit human intelligence. So that we must keep it and cultivate AIs to view the rules as overall principles as we do when they make decisions. To achieve this, a feasible is to restrict AIs in a simulated environment until scientists can mathematically prove that they accords to human welfare (Roman 219). In the case that AIs would break the rules, it's important to keep AIs' involvement with our world in a controllable range. We definitely can let them help since they are designed to do so, but we must do the major part. This works based on asking safe questions with restricted answers. For example, asking AI to pick a more possible solution based from two solutions which scientists think have equivalent possibility to work based on its computation. A group of experts should be trained to be familiar with the design of AI and the confinement environment so that they can review the question which is about to be asked and make sure it is safe (Roman 221). By this method, an abuse of AI can also be avoided. 
	Once we have built artificial intelligences, we want them to be safe. That is to say, robot rights will never be under consideration. AIs are not necessarily consciousness and it's obviously ridiculous to give an unconscious tool rights. Even if AIs finally get the point of consciousness, they are still tools created by humans. They have the obligation to serve us like what other machines do. Moreover, we will never expect our AIs behaves in the same way as we do. It is obvious because we don't want to design a machine that views its own survival as a terminal value and take advantage of every resource to protect its existence (Roman 223). Once AIs are potentially dangerous. We will never care about their rights because self-protecting is the terminal goal of humans. 
	Although there are some experts argue that the best way to eliminate the danger of AIs is to stop the research, it will never happen. Research in artificial intelligence did have some winter periods when both British and U.S. governments restricted AI research by policy and restoring funds. However, it only takes six years for scientist to exit the first winter and three years for the second one. Humans will never stop innovation and evolution in technology because of infinite desire and passion towards knowing the unknowns. For a well-known example, research in nuclear power resource doesn't stop even after the famous explosion in Chernobyl Nuclear Power Plant. 
Work Cited 
Yampolskiy, Roman, and Joshua Fox. "Safety Engineering for Artificial General 
Intelligence." Topoi 32. 2 (2012): 217-26. Springer Link. Web. 23 Nov. 2015. 
[name] , Personal Interview, 23 October, 2015 
Peter, Norvig. "Artificial Intelligence." New Scientist 216. 2889 (2012): Pi-8.8p. 	 
EBSCOhost. Web. 23 Nov. 2015. 
Bohannon, John, ed. "Fears of an AI Pioneer." Science 349. 6245 (2015): 252. Science. 
Web. 5 Dec. 2015. 
Baumeister, Roy F., E.J. Masicampo, and C. Nathan De Wall. "Arguing, Reasoning, and 
the Interpersonal (cultural) Functions of Human Consciousness." Behavioral and Brain Sciences 34. 02 (2011): 74. Behavioral and Brain Sciences. Web. 5 Dec. 2015. 
1 Artificial intelligence are machine that are able to make the correct decision in uncertain environments (Norvig para.14). 
2 Every time a user correct a translation or rescue an email from the junk file, the system will update its library automatically so that programmers don't need to recode it every time new information is added (Norvig para. 29). 
3 They should be tools that are able to take as variable factors as possible into account and solve certain problems. 
--------------- 
------------------------------------------------------------ 
--------------- 
------------------------------------------------------------ 
